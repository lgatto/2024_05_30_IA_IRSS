#+TITLE: On cats, farts and parrots.


I was invited to contribute to a [[https://www.linkedin.com/feed/update/urn:li:activity:7201099969779351552/][seminar/discussion]] on AI, and using
language models in research. These are the notes I prepared my short
presentation.

* Who am I

I feel it is important to say a few words to put my talk into
perspective:

- I am a [[https://lgatto.github.io/about/][Computational Biologist]], heading the [[https://lgatto.github.io/cbio-lab/][CBIO lab]] at the [[https://www.deduveinstitute.be/research-group/laurent-gatto][de Duve
  Institute]], [[https://uclouvain.be/][UCLouvain]]. We use/develop of DL in the CBIO lab. I am
  not an expert in IA.
- I have never used ChatGPT or any similar tools, and I'll tell you
  why! I however actively follow discussions on IA and its impact on
  society.
- I will focus on ChatGTP and similar LMs that are released by large
  and very powerful commercial entities for wide public consumption. I
  am not focusing on their application in research.

* Reference

If there's one reference to remember, it is this seminal paper from
March 2021, published in the Proceedings 2021 ACM Conference on
Fairness, Accountability, and Transparency (FAccT '21)

*[[https://dl.acm.org/doi/10.1145/3442188.3445922][On the Dangers of Stochastic Parrots: Can Language Models Be Too
Big?]]* by [[https://faculty.washington.edu/ebender/][Emily M. Bender]], [[https://en.wikipedia.org/wiki/Timnit_Gebru][Timnit Gebru]], [[https://linguistics.washington.edu/people/angelina-y-mcmillan-major][Angelina McMillan-Major]],
[[http://www.m-mitchell.com/][Shmargaret Shmitchell]].


- Emily Bender is a professor of computational linguistics at
  Washington University. Angelina McMillan-Major is/was a PhD student
  in her lab.
- Timnit Gebru is one of the most well-known and respected Black
  female scientists working in AI. She was a co-lead of Google's
  ethical AI research team. In December 2020, her employment with
  Google ended after Google management asked her to either withdraw
  the paper before publication, or remove the names of all the Google
  employees from the paper.
- Margaret Mitchell was later also fired from Google.

More about the [[https://en.wikipedia.org/wiki/Stochastic_parrot][Stochastic parrot]] paper here.

* When can I use ChatGPT?

Following [[https://webperso.info.ucl.ac.be/~yde/bio.html][Yves Deville]] and [[https://scholar.google.com/citations?user=uwMMh-YAAAAJ][Christine Jacqmot]]'s recommendations here.

** Given that

LLMs have absolutely no notion of "true" or "false", nor any
understanding of what it is asked.

** Use it if

1. You don't can about the validity of the results.
2. You are an expert in the field.

Note: lots has been said about ChatGPT's "occasional" hallucinations
(beware of the *anthropomorphising* word here). They always
hallucinate. It just happens so that sometimes, what is made up, is
not wrong. I will come back to some of these points in the later
*stochastic parrot* section.

* At what cost?

Many have tested ChatGPT. Some ([[https://www.bbc.com/news/articles/c511x4g7x7jo][New AI tools much hyped but not much
used, study says)]] possibly make regular use of the free and/or the
paid version. It might be used for important or minor/mundane
tasks. But at what costs?

** Human cost

Human cost is real and current. It is not a potential science-fiction
picture of AI vs humanity. Such a picture diminishes the current human
cost of AI, as is force-fed by big tech.

Here are some investigations about worker exploitation at OpenAI and
Google with respect to the curation of AI-generated content:

- [[https://time.com/6247678/openai-chatgpt-kenya-workers/][TIME]]: OpenAI Used Kenyan Workers on Less Than $2 Per Hour.
- [[https://www.theguardian.com/technology/2023/aug/02/ai-chatbot-training-human-toll-content-moderator-meta-openai][The Guardian]]: ‘It’s destroyed me completely’: Kenyan moderators
  decry toll of training of AI models.
- [[https://www.businessinsider.com/openai-kenyan-contract-workers-label-toxic-content-chatgpt-training-report-2023-1][Business Insider]]: Kenyan Workers Paid $2/hr Labeled Horrific Content
  for OpenAI.
- [[https://fortune.com/2024/05/03/google-search-raters-wages-benefits-contractors-tech-ai-employment/][Fortune]]: I’m paid $14 an hour to rate AI-generated Google search
  results. Subcontractors like me do key work but don’t get fair wages
  or benefits.

Note that this isn't specific to ChatGPT. Similar workers exploitation
has been documented for Meta/Facebook reviewers from the Global South.

*Already marginalised communities will suffer the highest human cost.*

** Environmental cost

Here are some relevant articles and illustrative quotes;

- [[https://www.nature.com/articles/s42256-020-0219-9][Nature Machine Intelligence]]: The carbon impact of artificial intelligence.
- [[https://www.technologyreview.com/2022/11/14/1063192/were-getting-a-better-idea-of-ais-true-carbon-footprint/][technologyreview.com]]: We’re getting a better idea of AI’s true
  carbon footprint.
- [[https://www.nature.com/articles/s41558-022-01377-7][Nature Climate Change]]: Aligning artificial intelligence with climate
  change mitigation.
- [[https://www.nature.com/articles/d41586-024-00478-x][nature.com]]: Generative AI’s environmental costs are soaring - and
  mostly secret.
- [[https://www.nature.com/articles/d41586-022-01983-7][nature.com]]: How to shrink AI’s ballooning carbon footprint.
- [[https://theconversation.com/ai-has-a-large-and-growing-carbon-footprint-but-there-are-potential-solutions-on-the-horizon-223488][theconversation.com]]: AI has a large and growing carbon footprint,
  but there are potential solution on the horizon.

  #+begin_src quote
  To put things in perspective, training GPT-3 (the precursor AI
  system to the current ChatGPT) generated 502 metric tonnes of
  carbon, which is equivalent to driving 112 petrol powered cars for a
  year. [...] GPT-3 further emits 8.4 tonnes of CO₂ annually due to
  inference. Since the AI boom started in the early 2010s, the energy
  requirements of AI systems known as large language models (LLMs) –
  the type of technology that’s behind ChatGPT – have gone up by a
  factor of 300,000. With the increasing ubiquity and complexity of AI
  models, this trend is going to continue, potentially making AI a
  significant contributor of CO₂ emissions. In fact, our current
  estimates could be lower than AI’s actual carbon footprint due to a
  lack of standard and accurate techniques for measuring AI-related
  emissions.
  #+end_src

- [[https://www.theguardian.com/technology/2023/jun/08/artificial-intelligence-industry-boom-environment-toll][The Guardian]]: As the AI industry booms, what toll will it take on
  the environment? (citing - [[https://arxiv.org/abs/2211.02001][Estimating the Carbon Footprint
  of BLOOM, a 176B Parameter Language Model]])

  #+begin_src quote
  [Luccioni et al.] tallied the amount of energy used to train [...]
  Bloom, on a supercomputer; the energy used to manufacture the
  supercomputer’s hardware and maintain its infrastructure; and the
  electricity used to run the program once it launched. They found
  that it generated about 50 metric tons of carbon dioxide emissions,
  the equivalent of an individual taking about 60 flights between
  London and New York.
  #+end_src

  #+begin_src quote
  By contrast, limited publicly available data suggests about 500
  metric tonnes of CO2 were produced just in the training of ChatGPT’s
  GPT3 model 3 – the equivalent of over a million miles driven by
  average gasoline-powered cars, the researchers noted.
  #+end_src

  #+begin_src quote
  Even more unclear is the amount of water consumed in the creation
  and use of various AI models. Data centers use water in evaporative
  cooling systems to keep equipment from overheating. One
  non-peer-reviewed study, led by researchers at UC Riverside,
  estimates that training GPT3 in Microsoft’s state-of-the-art US data
  centers could potentially have consumed 700,000 liters of
  freshwater.
  #+end_src

- [[https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-may-eventually-consume-a-quarter-of-americas-power-by-2030-warns-arm-ceo][tomshardware.com]]: AI may eventually consume a quarter of America's
  power by 2030, warns Arm CEO.
- [[https://www.bloomberg.com/news/articles/2024-05-15/microsoft-s-ai-investment-imperils-climate-goal-as-emissions-jump-30][bloomberg.com]]: Microsoft’s AI Investment Imperils Climate Goal As
  Emissions Jump 30%.

  How ironic!!

  #+begin_src quote
  "The company’s goal to be carbon negative by 2030 is harder to
  reach, but President Brad Smith says the good AI can do for the
  world will outweigh its environmental impact."
  #+end_src

Note that this is also relevant for other cloud services, such as
video on demande (detail for [[https://theconversation.com/que-sait-on-des-impacts-environnementaux-de-la-video-en-ligne-lexemple-de-netflix-229955][Netflix]] here).

*Already marginalised communities will suffer the highest climatic
cost.*

** Intellectual property

Where does all that training data come from?

- What about the credit and licensing of text, voice and images of
  those that produced that data used for training.

* Stochastic parrot

I'll borrow here directly from the paper, to highlight specific issues
with the vast amounts of data needed to train these large models, and
the (absence of) meaning output by the models.

** Unfathomable training data

- Size doesn't guarantee diversity: from initial participation, to
  data filtering, the data reflect the hegemonic viewpoint.
- Data is static data, but social views change.
- Biais is encoding and amplified in the training data, in particular
  stereotypical associations and negative sentiment towards specific
  groups.
- Large data and the lack of curation, documentation and
  accountability lead to a major documentation debt, that can't be
  addressed after the fact.

*Systematic biais againsy already marginalised communities.*

** Stochastic parrot

> Coherence is in the eye of the beholder

- There is no meaning, no model of the world, no intend to communicate
  in ChatGPT's output.
- Perceived "fluency" and "confidence" give the illusion of (implicit)
  meaning and expertise.
- We tend to mistake the seamingful coherence of LLM outputs for
  meaningful text or expertise.

#+begin_src quote
Contrary to how it may seem when we observe its output, an LM is a
system for haphazardly stitching together sequences of linguistic
forms it has observed in its vast training data, according to
probabilistic information about how they combine, but without any
reference to meaning: a *stochastic parrot*.
#+end_src

It is important to note that, in addition to highlight the risks, the
authors do propose paths forward for LM research and development.

* AI contamination

AI-generated text is already ubiquitous on-line, and it becomes more
and more difficult to identify AI-generated text. How long until
AI-generetaed (meaningless) text (including as answers in Q&A sites),
will be (or are) re-used for training.

Outlets are terminating journalist contract to replace them by AI, and
independent writers are 'competing' against AI.

We have all faced AI chat-bots in so-called help-desks. But [[https://theconversation.com/ai-chatbots-are-intruding-into-online-communities-where-people-are-trying-to-connect-with-other-humans-229473][AI
chatbots are intruding into online communities where people are trying
to connect with other humans]].

> Both of these responses were lies. That child does not exist and
neither do the camera or air conditioner. The answers came from an
artificial intelligence chatbot.

> According to a Meta help page, Meta AI will respond to a post in a
group if someone explicitly tags it or if someone “asks a question in
a post and no one responds within an hour.”

There are prime examples of *enshittification*:

> Enshittification is the pattern of decreasing quality observed in
> online services and products such as Amazon, Facebook, Google
> Search, Twitter, Bandcamp, Reddit, Uber, and Unity. The term was
> used by writer Cory Doctorow in November 2022, and the American
> Dialect Society selected it as its 2023 Word of the Year. Doctorow
> has also used the term platform decay to describe the same
> concept. (from [[https://en.wikipedia.org/wiki/Enshittification][Wikipedia]])

* ChatGPT in research

- Reproducibility? [[https://www.nature.com/articles/d41586-024-01463-0][AlphaFold3 — why did Nature publish it without its
  code?]]

  > When AlphaFold2 was published2, the full underlying code was made
  accessible to all researchers. But AlphaFold3 comes with
  ‘pseudocode’ — a detailed description of what the code can do and
  how it works.

  > [...] for AlphaFold2, the DeepMind team worked with the European
  Molecular Biology Laboratory’s European Bioinformatics Institute
  [...] Now, DeepMind has partnered with Isomorphic Labs, a
  London-based drug-development company owned by Google’s parent,
  Alphabet. In addition to the non-availability of the full code,
  there are other restrictions on the use of the tool — for example,
  in drug development. There are also daily limits on the numbers of
  predictions that individual researchers can perform.

- [[https://www.theguardian.com/science/2023/jan/26/science-journals-ban-listing-of-chatgpt-as-co-author-on-papers][Science journals ban listing of ChatGPT as co-author on papers]]

- Paper writing (paper mills) and reviews (ChatGPT is
  [[https://www.nature.com/articles/d41586-024-01106-4][polluting]]/[[https://arxiv.org/abs/2405.02150][influencing]] peer review).

* Who benefits from ChatGTP/AI?

... being is force-fed by big tech!

- In search engines? Not the users.
- Facial recognition. Not the citizens.
- Microsoft Windows Recall. Not the employees.

*Already marginalised communities to benefit the least. Privileged
communities to benefit the most.*

* What about regulations?

- [[https://www.theguardian.com/technology/article/2024/may/28/openai-safety-council-chatgpt?CMP=Share_AndroidApp_Other][OpenAI forms safety council as it trains latest artificial
  intelligence model]]: The safety committee is filled with company
  insiders, including Sam Altman, the OpenAI CEO, and its chairman,
  Bret Taylor, and four OpenAI technical and policy experts. It also
  includes the board members Adam D’Angelo, who is the CEO of Quora,
  and Nicole Seligman, a former Sony general counsel.

- [[https://theintercept.com/2019/12/20/mit-ethical-ai-artificial-intelligence/][How Big Tech Manipulates Academia to Avoid Regulation]]: The discourse
  of “ethical AI” was aligned strategically with a Silicon Valley
  effort seeking to avoid legally enforceable restrictions of
  controversial technologies.

* Conclusions

Despite some notable failures with 'AI for public consumption' , one
can't ignore that there there are also success stories, and possibly
still untapped opportunities. But ...

> AI can be kind of useful, but I'm not sure that a "kind of useful"
> tool justifies the harm.

[[https://www.citationneeded.news/ai-isnt-useless/][AI isn't useless. But is it worth it?]], Molly White
